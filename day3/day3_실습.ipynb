{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6852e1d7",
   "metadata": {},
   "source": [
    "# all call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4f2a30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install bs4 langchain_community langchainhub chromadb langchain langgraph tavily-python langchain-text-splitters langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fbac5201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96179658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tavily import TavilyClient\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "111b6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "## retriever\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf77a9",
   "metadata": {},
   "source": [
    "### 실습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4285960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "exmaple: retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
    "-> {\"score\": \"yes\" or \"no\"}\n",
    "\"\"\"\n",
    "\n",
    "_doc_retriever = retriever\n",
    "_relevance_checker = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \n",
    "         \"\"\"You are a grader assessing relevance\n",
    "            of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
    "            grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "            Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "            Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "        \"\"\"),\n",
    "        (\"human\", \"question: {question}\\n\\n document: {document} \"),\n",
    "    ]\n",
    ") | llm | JsonOutputParser()\n",
    "\n",
    "_answer_generator = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \n",
    "         \"\"\"You are an assistant for question-answering tasks.\n",
    "            Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "            Use three sentences maximum and keep the answer concise\"\"\"\n",
    "        ),\n",
    "        (\"human\", \"question: {question}\\n\\n context: {context} \"),\n",
    "    ]\n",
    ") | llm | StrOutputParser()\n",
    "\n",
    "_hallucination_checker = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \n",
    "         \"\"\"You are a grader assessing whether\n",
    "            an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate\n",
    "            whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n",
    "            single key 'score' and no preamble or explanation.\"\"\"),\n",
    "        (\"human\", \"documents: {documents}\\n\\n answer: {generation} \"),\n",
    "    ]\n",
    ") | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "feaf7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "### nodes\n",
    "\n",
    "def docs_retrieval(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = _doc_retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def relevance_checker(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    doc_txt = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    result = _relevance_checker.invoke({\"question\": question, \"document\": doc_txt})\n",
    "    return {\"documents\": documents, \"question\": question, \"relevance\": result[\"score\"]}\n",
    "\n",
    "def generator_answer(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    doc_txt = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    result = _answer_generator.invoke({\"question\": question, \"context\": doc_txt})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": result}\n",
    "\n",
    "def hallucination_checker(state):\n",
    "    generation = state[\"generation\"]\n",
    "    documents = state[\"documents\"]\n",
    "    try_count = state.get(\"try_count\", 0)\n",
    "    result = _hallucination_checker.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    \n",
    "    if \"try_count\" in state:\n",
    "        state[\"try_count\"] += 1\n",
    "    else:\n",
    "        state[\"try_count\"] = 1\n",
    "    \n",
    "    if try_count > 2:\n",
    "        hallucination = \"failed\"\n",
    "    else:\n",
    "        hallucination = result[\"score\"]\n",
    "    \n",
    "    return {\n",
    "        \"documents\": documents, \n",
    "        \"generation\": generation, \n",
    "        \"hallucination\": hallucination, \n",
    "        \"try_count\": try_count}\n",
    "    \n",
    "def web_search(state):\n",
    "    question = state[\"question\"]\n",
    "    docs = tavily.search(query=question)['results']\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    return {\"documents\": [web_results], \"question\": question}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ea8c7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x11b99e390>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### State\n",
    "from langgraph.graph import END, StateGraph\n",
    "from typing import List, Literal, TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "    relevance: Literal[\"yes\", \"no\"]\n",
    "    hallucination: Literal[\"yes\", \"no\"]\n",
    "    try_count: int\n",
    "\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"docs_retrieval\", docs_retrieval)\n",
    "workflow.add_node(\"relevance_checker\", relevance_checker)\n",
    "workflow.add_node(\"generator_answer\", generator_answer) \n",
    "workflow.add_node(\"hallucination_checker\", hallucination_checker)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "\n",
    "workflow.set_entry_point(\"docs_retrieval\")\n",
    "workflow.add_edge(\"docs_retrieval\", \"relevance_checker\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"relevance_checker\",\n",
    "    lambda state: state[\"relevance\"],\n",
    "    {\n",
    "        \"yes\": \"generator_answer\",\n",
    "        \"no\": \"web_search\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"relevance_checker\")\n",
    "workflow.add_edge(\"generator_answer\", \"hallucination_checker\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"hallucination_checker\",\n",
    "    lambda state: state[\"hallucination\"],\n",
    "    {\n",
    "        \"yes\": END,\n",
    "        \"no\": \"generator_answer\",\n",
    "        \"failed\": END,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "34b23dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Finished running: docs_retrieval:'\n",
      "'Finished running: relevance_checker:'\n",
      "'Finished running: web_search:'\n",
      "'Finished running: relevance_checker:'\n",
      "'Finished running: generator_answer:'\n",
      "'Finished running: hallucination_checker:'\n",
      "('Lionel Messi currently plays for Inter Miami. He is involved in the MLS '\n",
      " 'season and will also play for Argentina.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "app = workflow.compile()\n",
    "inputs = {\"question\": \"Where does Messi play right now?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "\n",
    "if value[\"hallucination\"] == \"failed\":\n",
    "    pprint(\"failed: hallucination\")\n",
    "else:\n",
    "    pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "day3-vki1Ew6j",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
